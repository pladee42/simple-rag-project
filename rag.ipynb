{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5AcfSQwuccj"
   },
   "source": [
    "# Overview\n",
    "\n",
    "This notebook focuses on:\n",
    "\n",
    "1. Retrieving relevant context from a research about RAG https://arxiv.org/pdf/2312.10997.\n",
    "2. Formatting prompts for Large Language Models (LLMs) using the retrieved context.\n",
    "3. Generating high-quality responses without fabricating unsupported details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jD_dZB5OCq7-"
   },
   "source": [
    "# 1. Read File and Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjxVpsik2pyy"
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "sxVQwmKD22hI",
    "outputId": "bb46ecce-cf33-48dd-eb7f-4ee2024a0138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting PyMuPDF\n",
      "  Using cached PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Using cached PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
      "Installing collected packages: PyMuPDF\n",
      "  Attempting uninstall: PyMuPDF\n",
      "    Found existing installation: PyMuPDF 1.24.14\n",
      "    Uninstalling PyMuPDF-1.24.14:\n",
      "      Successfully uninstalled PyMuPDF-1.24.14\n",
      "Successfully installed PyMuPDF-1.24.14\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.26.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.7.0.post2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.1+cu121)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
     ]
    }
   ],
   "source": [
    "# Google Colab installs\n",
    "import os\n",
    "\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "    !pip install -U torch\n",
    "    !pip install --upgrade --force-reinstall PyMuPDF # for reading PDFs with Python\n",
    "    !pip install sentence-transformers # for embedding models\n",
    "    !pip install accelerate # for quantization model loading\n",
    "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
    "    !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference\n",
    "    !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "XUKrXdx22py1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import fitz\n",
    "from spacy.lang.en import English\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGP6VVsTQcHQ"
   },
   "source": [
    "Define Torch's Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "Yj_RGKJDQcHQ"
   },
   "outputs": [],
   "source": [
    "if \"COLAB_GPU\" in os.environ:\n",
    "   device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = 'mps' # For MacOS\n",
    "else:\n",
    "   device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7Xx9jKs2py2"
   },
   "source": [
    "## Read the PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCtB4OXO693r",
    "outputId": "8802ae51-922f-4281-a9ca-7102ca0d4109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File RAG for LLM.pdf exists.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = 'RAG for LLM.pdf'\n",
    "url = 'https://arxiv.org/pdf/2312.10997'\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Open a file in binary write mode and save the content to it\n",
    "        with open(pdf_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"The file has been downloaded and saved as {pdf_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(f\"File {pdf_path} exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNczlovZ2py2",
    "outputId": "61ce1f2e-147e-46c9-c7a7-f43cc7ea2310"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "pdf_file = []\n",
    "for page_number, page in enumerate(doc):  # iterate the document pages\n",
    "    text = page.get_text()  # get plain text encoded as UTF-8\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    pdf_file.append({\"page_number\": page_number,\n",
    "                            \"page_char_count\": len(text),\n",
    "                            \"page_word_count\": len(text.split(\" \")),\n",
    "                            \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                            \"page_token_count\": len(text) / 4,\n",
    "                            \"text\": text})\n",
    "len(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "4UZnxqmp2py3",
    "outputId": "ee591995-ef73-4757-df8d-73031bcdfd22"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 21,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 0,\n        \"max\": 20,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0,\n          17,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2304,\n        \"min\": 517,\n        \"max\": 9073,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          5451,\n          8997,\n          3641\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 337,\n        \"min\": 77,\n        \"max\": 1302,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          739,\n          1302,\n          496\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 121,\n        \"min\": 1,\n        \"max\": 338,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          28,\n          32,\n          46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 576.2083842528963,\n        \"min\": 129.25,\n        \"max\": 2268.25,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          1362.75,\n          2249.25,\n          910.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          \"1 Retrieval-Augmented Generation for Large Language Models: A Survey Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng Wangc, and Haofen Wang a,c aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University cCollege of Design and Innovation, Tongji University Abstract\\u2014Large Language Models (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain- specific information. RAG synergistically merges LLMs\\u2019 intrin- sic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evalua- tion framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development 1. Index Terms\\u2014Large language model, retrieval-augmented gen- eration, natural language processing, information retrieval I. INTRODUCTION L ARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks [1], notably producing \\u201challucinations\\u201d [2] when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applica- tions. RAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown Corresponding Author.Email:haofen.wang@tongji.edu.cn 1Resources are available at https://github.com/Tongji-KGLLM/ RAG-Survey in Figure 1. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG\\u2019s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating additional knowledge through Pre- Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques [3]\\u2013[5].The subsequent arrival of ChatGPT [6] marked a pivotal moment, with LLM demonstrating powerful in context learning (ICL) capabilities. RAG research shifted towards providing better information for LLMs to answer more com- plex and knowledge-intensive tasks during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques. The burgeoning field of RAG has experienced swift growth, yet it has not been accompanied by a systematic synthesis that could clarify its broader trajectory. This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of \\u201cRetrieval,\\u201d \\u201cGeneration,\\u201d and \\u201cAugmentation.\\u201d On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper compre- hensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: \\u2022 In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, arXiv:2312.10997v5  [cs.CL]  27 Mar 2024\",\n          \"18 [44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong, O. Kuchaiev, B. Li, C. Xiao et al., \\u201cShall we pretrain autoregressive language models with retrieval? a comprehensive study,\\u201d arXiv preprint arXiv:2304.06762, 2023. [45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan- zaro, \\u201cInstructretro: Instruction tuning post retrieval-augmented pre- training,\\u201d arXiv preprint arXiv:2310.07713, 2023. [46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana, and S. Nanayakkara, \\u201cImproving the domain adaptation of retrieval augmented generation (rag) models for open domain question answer- ing,\\u201d Transactions of the Association for Computational Linguistics, vol. 11, pp. 1\\u201317, 2023. [47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, \\u201cAugmentation-adapted retriever improves generalization of language models as generic plug-in,\\u201d arXiv preprint arXiv:2305.17331, 2023. [48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, \\u201cMaking retrieval- augmented language models robust to irrelevant context,\\u201d arXiv preprint arXiv:2310.01558, 2023. [49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, \\u201cUnderstanding re- trieval augmentation for long-form question answering,\\u201d arXiv preprint arXiv:2310.12150, 2023. [50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, \\u201cChain-of-note: Enhancing robustness in retrieval-augmented language models,\\u201d arXiv preprint arXiv:2311.09210, 2023. [51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, \\u201cSearch-in-the- chain: Towards accurate, credible and traceable large language models for knowledgeintensive tasks,\\u201d CoRR, vol. abs/2304.14732, 2023. [52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat, \\u201cOptimizing retrieval-augmented reader models via token elimination,\\u201d arXiv preprint arXiv:2310.13682, 2023. [53] J. L\\u00b4ala, O. O\\u2019Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques, and A. D. White, \\u201cPaperqa: Retrieval-augmented generative agent for scientific research,\\u201d arXiv preprint arXiv:2312.07559, 2023. [54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano, Y. Maarek, N. Tonellotto, and F. Silvestri, \\u201cThe power of noise: Redefining retrieval for rag systems,\\u201d arXiv preprint arXiv:2401.14887, 2024. [55] Z. Zhang, X. Zhang, Y. Ren, S. Shi, M. Han, Y. Wu, R. Lai, and Z. Cao, \\u201cIag: Induction-augmented generation framework for answer- ing reasoning questions,\\u201d in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 1\\u201314. [56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo, D. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al., \\u201cNomiracl: Knowing when you don\\u2019t know for robust multilingual retrieval-augmented generation,\\u201d arXiv preprint arXiv:2312.11361, 2023. [57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, \\u201cTree of clarifica- tions: Answering ambiguous questions with retrieval-augmented large language models,\\u201d arXiv preprint arXiv:2310.14696, 2023. [58] Y. Wang, P. Li, M. Sun, and Y. Liu, \\u201cSelf-knowledge guided retrieval augmentation for large language models,\\u201d arXiv preprint arXiv:2310.05002, 2023. [59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, \\u201cRetrieval- generation synergy augmented large language models,\\u201d arXiv preprint arXiv:2310.05149, 2023. [60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian, E. Bakhturina, M. Shoeybi, and B. Catanzaro, \\u201cRetrieval meets long context large language models,\\u201d arXiv preprint arXiv:2310.03025, 2023. [61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, \\u201cInterleav- ing retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions,\\u201d arXiv preprint arXiv:2212.10509, 2022. [62] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.- R. Wen, and H. Wang, \\u201cInvestigating the factual knowledge boundary of large language models with retrieval augmentation,\\u201d arXiv preprint arXiv:2307.11019, 2023. [63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D. Manning, \\u201cRaptor: Recursive abstractive processing for tree-organized retrieval,\\u201d arXiv preprint arXiv:2401.18059, 2024. [64] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton- Brown, and Y. Shoham, \\u201cIn-context retrieval-augmented language models,\\u201d arXiv preprint arXiv:2302.00083, 2023. [65] Y. Ren, Y. Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, \\u201cRetrieve-and- sample: Document-level event argument extraction via hybrid retrieval augmentation,\\u201d in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 293\\u2013306. [66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, \\u201cZemi: Learning zero-shot semi-parametric language models from multiple tasks,\\u201d arXiv preprint arXiv:2210.00185, 2022. [67] S.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling, \\u201cCorrective retrieval augmented generation,\\u201d arXiv preprint arXiv:2401.15884, 2024. [68] P. Jain, L. B. Soares, and T. Kwiatkowski, \\u201c1-pager: One pass answer generation and evidence retrieval,\\u201d arXiv preprint arXiv:2310.16568, 2023. [69] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, \\u201cPrca: Fitting black-box large language models for retrieval question answer- ing via pluggable reward-driven contextual adapter,\\u201d arXiv preprint arXiv:2310.18347, 2023. [70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, \\u201cOpen-source large language models are strong zero-shot query likelihood models for document ranking,\\u201d arXiv preprint arXiv:2310.13243, 2023. [71] F. Xu, W. Shi, and E. Choi, \\u201cRecomp: Improving retrieval-augmented lms with compression and selective augmentation,\\u201d arXiv preprint arXiv:2310.04408, 2023. [72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle- moyer, and W.-t. Yih, \\u201cReplug: Retrieval-augmented black-box lan- guage models,\\u201d arXiv preprint arXiv:2301.12652, 2023. [73] E. Melz, \\u201cEnhancing llm intelligence with arm-rag: Auxiliary ra- tionale memory for retrieval augmented generation,\\u201d arXiv preprint arXiv:2311.04177, 2023. [74] H. Wang, W. Huang, Y. Deng, R. Wang, Z. Wang, Y. Wang, F. Mi, J. Z. Pan, and K.-F. Wong, \\u201cUnims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems,\\u201d arXiv preprint arXiv:2401.13256, 2024. [75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang, \\u201cAugmented large language models with parametric knowledge guid- ing,\\u201d arXiv preprint arXiv:2305.04757, 2023. [76] X. Li, Z. Liu, C. Xiong, S. Yu, Y. Gu, Z. Liu, and G. Yu, \\u201cStructure- aware language model pretraining improves dense retrieval on struc- tured data,\\u201d arXiv preprint arXiv:2305.19912, 2023. [77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, \\u201cKnowledge graph-augmented language models for knowledge-grounded dialogue generation,\\u201d arXiv preprint arXiv:2305.18846, 2023. [78] W. Shen, Y. Gao, C. Huang, F. Wan, X. Quan, and W. Bi, \\u201cRetrieval- generation alignment for end-to-end task-oriented dialogue system,\\u201d arXiv preprint arXiv:2310.08877, 2023. [79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, \\u201cDual-feedback knowledge retrieval for task-oriented dialogue systems,\\u201d arXiv preprint arXiv:2310.14528, 2023. [80] P. Ranade and A. Joshi, \\u201cFabula: Intelligence report generation using retrieval-augmented narrative construction,\\u201d arXiv preprint arXiv:2310.13848, 2023. [81] X. Jiang, R. Zhang, Y. Xu, R. Qiu, Y. Fang, Z. Wang, J. Tang, H. Ding, X. Chu, J. Zhao et al., \\u201cThink and retrieval: A hypothesis knowledge graph enhanced medical large language models,\\u201d arXiv preprint arXiv:2312.15883, 2023. [82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang, \\u201cKnowledge-augmented language model verification,\\u201d arXiv preprint arXiv:2310.12836, 2023. [83] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, \\u201cReasoning on graphs: Faithful and interpretable large language model reasoning,\\u201d arXiv preprint arXiv:2310.01061, 2023. [84] X. He, Y. Tian, Y. Sun, N. V. Chawla, T. Laurent, Y. LeCun, X. Bresson, and B. Hooi, \\u201cG-retriever: Retrieval-augmented generation for textual graph understanding and question answering,\\u201d arXiv preprint arXiv:2402.07630, 2024. [85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su, X. Li, A. Su et al., \\u201cTablegpt: Towards unifying tables, nature language and commands into one gpt,\\u201d arXiv preprint arXiv:2307.08674, 2023. [86] M. Gaur, K. Gunaratna, V. Srinivasan, and H. Jin, \\u201cIseeq: Information seeking question generation using dynamic meta-information retrieval and knowledge graphs,\\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 10, 2022, pp. 10 672\\u201310 680. [87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch\\u00a8arli, and D. Zhou, \\u201cLarge language models can be easily distracted by irrelevant context,\\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 31 210\\u201331 227. [88] R. Teja, \\u201cEvaluating the ideal chunk size for a rag system using llamaindex,\\u201d https://www.llamaindex.ai/blog/ evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5, 2023.\",\n          \"16 Fig. 6. Summary of RAG ecosystem initial learning curve. 3) Specialization - optimizing RAG to better serve production environments. The mutual growth of RAG models and their technology stacks is evident; technological advancements continuously establish new standards for existing infrastructure. In turn, enhancements to the technology stack drive the development of RAG capabilities. RAG toolkits are converging into a foundational technology stack, laying the groundwork for advanced enterprise applications. However, a fully integrated, comprehensive platform concept is still in the future, requiring further innovation and development. F. Multi-modal RAG RAG has transcended its initial text-based question- answering confines, embracing a diverse array of modal data. This expansion has spawned innovative multimodal models that integrate RAG concepts across various domains: Image. RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images. BLIP-2 [177] leverages frozen image encoders alongside LLMs for efficient visual language pre-training, enabling zero- shot image-to-text conversions. The \\u201cVisualize Before You Write\\u201d method [178] employs image generation to steer the LM\\u2019s text generation, showing promise in open-ended text generation tasks. Audio and Video. The GSS method retrieves and stitches together audio clips to convert machine-translated data into speech-translated data [179]. UEOP marks a significant ad- vancement in end-to-end automatic speech recognition by incorporating external, offline strategies for voice-to-text con- version [180]. Additionally, KNN-based attention fusion lever- ages audio embeddings and semantically related text embed- dings to refine ASR, thereby accelerating domain adaptation. Vid2Seq augments language models with specialized temporal markers, facilitating the prediction of event boundaries and textual descriptions within a unified output sequence [181]. Code. RBPS [182] excels in small-scale learning tasks by retrieving code examples that align with developers\\u2019 objectives through encoding and frequency analysis. This approach has demonstrated efficacy in tasks such as test assertion genera- tion and program repair. For structured knowledge, the CoK method [106] first extracts facts pertinent to the input query from a knowledge graph, then integrates these facts as hints within the input, enhancing performance in knowledge graph question-answering tasks. VIII. CONCLUSION The summary of this paper, as depicted in Figure 6, empha- sizes RAG\\u2019s significant advancement in enhancing the capa- bilities of LLMs by integrating parameterized knowledge from language models with extensive non-parameterized data from external knowledge bases. The survey showcases the evolution of RAG technologies and their application on many different tasks. The analysis outlines three developmental paradigms within the RAG framework: Naive, Advanced, and Modu- lar RAG, each representing a progressive enhancement over its predecessors. RAG\\u2019s technical integration with other AI methodologies, such as fine-tuning and reinforcement learning, has further expanded its capabilities. Despite the progress in RAG technology, there are research opportunities to improve its robustness and its ability to handle extended contexts. RAG\\u2019s application scope is expanding into multimodal do- mains, adapting its principles to interpret and process diverse data forms like images, videos, and code. This expansion high- lights RAG\\u2019s significant practical implications for AI deploy- ment, attracting interest from academic and industrial sectors.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-77facd4a-7ae4-4302-bfe1-636ce0285a22\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5451</td>\n",
       "      <td>739</td>\n",
       "      <td>28</td>\n",
       "      <td>1362.75</td>\n",
       "      <td>1 Retrieval-Augmented Generation for Large Lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3082</td>\n",
       "      <td>450</td>\n",
       "      <td>32</td>\n",
       "      <td>770.50</td>\n",
       "      <td>2 Fig. 1. Technology tree of RAG research. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3950</td>\n",
       "      <td>575</td>\n",
       "      <td>41</td>\n",
       "      <td>987.50</td>\n",
       "      <td>3 Fig. 2. A representative instance of the RAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3574</td>\n",
       "      <td>492</td>\n",
       "      <td>30</td>\n",
       "      <td>893.50</td>\n",
       "      <td>4 Fig. 3. Comparison between the three paradig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6362</td>\n",
       "      <td>887</td>\n",
       "      <td>42</td>\n",
       "      <td>1590.50</td>\n",
       "      <td>5 aligns the text more closely with data distr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77facd4a-7ae4-4302-bfe1-636ce0285a22')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-77facd4a-7ae4-4302-bfe1-636ce0285a22 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-77facd4a-7ae4-4302-bfe1-636ce0285a22');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-005dca29-7eca-484d-affe-f820f41a24db\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-005dca29-7eca-484d-affe-f820f41a24db')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-005dca29-7eca-484d-affe-f820f41a24db button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0            0             5451              739                       28   \n",
       "1            1             3082              450                       32   \n",
       "2            2             3950              575                       41   \n",
       "3            3             3574              492                       30   \n",
       "4            4             6362              887                       42   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0           1362.75  1 Retrieval-Augmented Generation for Large Lan...  \n",
       "1            770.50  2 Fig. 1. Technology tree of RAG research. The...  \n",
       "2            987.50  3 Fig. 2. A representative instance of the RAG...  \n",
       "3            893.50  4 Fig. 3. Comparison between the three paradig...  \n",
       "4           1590.50  5 aligns the text more closely with data distr...  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pdf_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mab8K8Y02py3"
   },
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7A1iJP62py4"
   },
   "source": [
    "### Sentencizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uu3_Vpvx2py4",
    "outputId": "f3747a5e-8033-4ff3-9105-fa149c326ffd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[First sentences., Second sentence.]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "# Test\n",
    "list(nlp(\"First sentences. Second sentence.\").sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "kWQF5HEP2py4",
    "outputId": "4d1edd23-9847-4e01-b7cc-10855221e579"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 21,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 0,\n        \"max\": 20,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0,\n          17,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2304,\n        \"min\": 517,\n        \"max\": 9073,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          5451,\n          8997,\n          3641\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 337,\n        \"min\": 77,\n        \"max\": 1302,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          739,\n          1302,\n          496\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 121,\n        \"min\": 1,\n        \"max\": 338,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          28,\n          32,\n          46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 576.2083842528963,\n        \"min\": 129.25,\n        \"max\": 2268.25,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          1362.75,\n          2249.25,\n          910.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          \"1 Retrieval-Augmented Generation for Large Language Models: A Survey Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng Wangc, and Haofen Wang a,c aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University cCollege of Design and Innovation, Tongji University Abstract\\u2014Large Language Models (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain- specific information. RAG synergistically merges LLMs\\u2019 intrin- sic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evalua- tion framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development 1. Index Terms\\u2014Large language model, retrieval-augmented gen- eration, natural language processing, information retrieval I. INTRODUCTION L ARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks [1], notably producing \\u201challucinations\\u201d [2] when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applica- tions. RAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown Corresponding Author.Email:haofen.wang@tongji.edu.cn 1Resources are available at https://github.com/Tongji-KGLLM/ RAG-Survey in Figure 1. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG\\u2019s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating additional knowledge through Pre- Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques [3]\\u2013[5].The subsequent arrival of ChatGPT [6] marked a pivotal moment, with LLM demonstrating powerful in context learning (ICL) capabilities. RAG research shifted towards providing better information for LLMs to answer more com- plex and knowledge-intensive tasks during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques. The burgeoning field of RAG has experienced swift growth, yet it has not been accompanied by a systematic synthesis that could clarify its broader trajectory. This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of \\u201cRetrieval,\\u201d \\u201cGeneration,\\u201d and \\u201cAugmentation.\\u201d On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper compre- hensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: \\u2022 In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, arXiv:2312.10997v5  [cs.CL]  27 Mar 2024\",\n          \"18 [44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong, O. Kuchaiev, B. Li, C. Xiao et al., \\u201cShall we pretrain autoregressive language models with retrieval? a comprehensive study,\\u201d arXiv preprint arXiv:2304.06762, 2023. [45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan- zaro, \\u201cInstructretro: Instruction tuning post retrieval-augmented pre- training,\\u201d arXiv preprint arXiv:2310.07713, 2023. [46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana, and S. Nanayakkara, \\u201cImproving the domain adaptation of retrieval augmented generation (rag) models for open domain question answer- ing,\\u201d Transactions of the Association for Computational Linguistics, vol. 11, pp. 1\\u201317, 2023. [47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, \\u201cAugmentation-adapted retriever improves generalization of language models as generic plug-in,\\u201d arXiv preprint arXiv:2305.17331, 2023. [48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, \\u201cMaking retrieval- augmented language models robust to irrelevant context,\\u201d arXiv preprint arXiv:2310.01558, 2023. [49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, \\u201cUnderstanding re- trieval augmentation for long-form question answering,\\u201d arXiv preprint arXiv:2310.12150, 2023. [50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, \\u201cChain-of-note: Enhancing robustness in retrieval-augmented language models,\\u201d arXiv preprint arXiv:2311.09210, 2023. [51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, \\u201cSearch-in-the- chain: Towards accurate, credible and traceable large language models for knowledgeintensive tasks,\\u201d CoRR, vol. abs/2304.14732, 2023. [52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat, \\u201cOptimizing retrieval-augmented reader models via token elimination,\\u201d arXiv preprint arXiv:2310.13682, 2023. [53] J. L\\u00b4ala, O. O\\u2019Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques, and A. D. White, \\u201cPaperqa: Retrieval-augmented generative agent for scientific research,\\u201d arXiv preprint arXiv:2312.07559, 2023. [54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano, Y. Maarek, N. Tonellotto, and F. Silvestri, \\u201cThe power of noise: Redefining retrieval for rag systems,\\u201d arXiv preprint arXiv:2401.14887, 2024. [55] Z. Zhang, X. Zhang, Y. Ren, S. Shi, M. Han, Y. Wu, R. Lai, and Z. Cao, \\u201cIag: Induction-augmented generation framework for answer- ing reasoning questions,\\u201d in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 1\\u201314. [56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo, D. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al., \\u201cNomiracl: Knowing when you don\\u2019t know for robust multilingual retrieval-augmented generation,\\u201d arXiv preprint arXiv:2312.11361, 2023. [57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, \\u201cTree of clarifica- tions: Answering ambiguous questions with retrieval-augmented large language models,\\u201d arXiv preprint arXiv:2310.14696, 2023. [58] Y. Wang, P. Li, M. Sun, and Y. Liu, \\u201cSelf-knowledge guided retrieval augmentation for large language models,\\u201d arXiv preprint arXiv:2310.05002, 2023. [59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, \\u201cRetrieval- generation synergy augmented large language models,\\u201d arXiv preprint arXiv:2310.05149, 2023. [60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian, E. Bakhturina, M. Shoeybi, and B. Catanzaro, \\u201cRetrieval meets long context large language models,\\u201d arXiv preprint arXiv:2310.03025, 2023. [61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, \\u201cInterleav- ing retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions,\\u201d arXiv preprint arXiv:2212.10509, 2022. [62] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.- R. Wen, and H. Wang, \\u201cInvestigating the factual knowledge boundary of large language models with retrieval augmentation,\\u201d arXiv preprint arXiv:2307.11019, 2023. [63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D. Manning, \\u201cRaptor: Recursive abstractive processing for tree-organized retrieval,\\u201d arXiv preprint arXiv:2401.18059, 2024. [64] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton- Brown, and Y. Shoham, \\u201cIn-context retrieval-augmented language models,\\u201d arXiv preprint arXiv:2302.00083, 2023. [65] Y. Ren, Y. Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, \\u201cRetrieve-and- sample: Document-level event argument extraction via hybrid retrieval augmentation,\\u201d in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 293\\u2013306. [66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, \\u201cZemi: Learning zero-shot semi-parametric language models from multiple tasks,\\u201d arXiv preprint arXiv:2210.00185, 2022. [67] S.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling, \\u201cCorrective retrieval augmented generation,\\u201d arXiv preprint arXiv:2401.15884, 2024. [68] P. Jain, L. B. Soares, and T. Kwiatkowski, \\u201c1-pager: One pass answer generation and evidence retrieval,\\u201d arXiv preprint arXiv:2310.16568, 2023. [69] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, \\u201cPrca: Fitting black-box large language models for retrieval question answer- ing via pluggable reward-driven contextual adapter,\\u201d arXiv preprint arXiv:2310.18347, 2023. [70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, \\u201cOpen-source large language models are strong zero-shot query likelihood models for document ranking,\\u201d arXiv preprint arXiv:2310.13243, 2023. [71] F. Xu, W. Shi, and E. Choi, \\u201cRecomp: Improving retrieval-augmented lms with compression and selective augmentation,\\u201d arXiv preprint arXiv:2310.04408, 2023. [72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle- moyer, and W.-t. Yih, \\u201cReplug: Retrieval-augmented black-box lan- guage models,\\u201d arXiv preprint arXiv:2301.12652, 2023. [73] E. Melz, \\u201cEnhancing llm intelligence with arm-rag: Auxiliary ra- tionale memory for retrieval augmented generation,\\u201d arXiv preprint arXiv:2311.04177, 2023. [74] H. Wang, W. Huang, Y. Deng, R. Wang, Z. Wang, Y. Wang, F. Mi, J. Z. Pan, and K.-F. Wong, \\u201cUnims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems,\\u201d arXiv preprint arXiv:2401.13256, 2024. [75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang, \\u201cAugmented large language models with parametric knowledge guid- ing,\\u201d arXiv preprint arXiv:2305.04757, 2023. [76] X. Li, Z. Liu, C. Xiong, S. Yu, Y. Gu, Z. Liu, and G. Yu, \\u201cStructure- aware language model pretraining improves dense retrieval on struc- tured data,\\u201d arXiv preprint arXiv:2305.19912, 2023. [77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, \\u201cKnowledge graph-augmented language models for knowledge-grounded dialogue generation,\\u201d arXiv preprint arXiv:2305.18846, 2023. [78] W. Shen, Y. Gao, C. Huang, F. Wan, X. Quan, and W. Bi, \\u201cRetrieval- generation alignment for end-to-end task-oriented dialogue system,\\u201d arXiv preprint arXiv:2310.08877, 2023. [79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, \\u201cDual-feedback knowledge retrieval for task-oriented dialogue systems,\\u201d arXiv preprint arXiv:2310.14528, 2023. [80] P. Ranade and A. Joshi, \\u201cFabula: Intelligence report generation using retrieval-augmented narrative construction,\\u201d arXiv preprint arXiv:2310.13848, 2023. [81] X. Jiang, R. Zhang, Y. Xu, R. Qiu, Y. Fang, Z. Wang, J. Tang, H. Ding, X. Chu, J. Zhao et al., \\u201cThink and retrieval: A hypothesis knowledge graph enhanced medical large language models,\\u201d arXiv preprint arXiv:2312.15883, 2023. [82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang, \\u201cKnowledge-augmented language model verification,\\u201d arXiv preprint arXiv:2310.12836, 2023. [83] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, \\u201cReasoning on graphs: Faithful and interpretable large language model reasoning,\\u201d arXiv preprint arXiv:2310.01061, 2023. [84] X. He, Y. Tian, Y. Sun, N. V. Chawla, T. Laurent, Y. LeCun, X. Bresson, and B. Hooi, \\u201cG-retriever: Retrieval-augmented generation for textual graph understanding and question answering,\\u201d arXiv preprint arXiv:2402.07630, 2024. [85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su, X. Li, A. Su et al., \\u201cTablegpt: Towards unifying tables, nature language and commands into one gpt,\\u201d arXiv preprint arXiv:2307.08674, 2023. [86] M. Gaur, K. Gunaratna, V. Srinivasan, and H. Jin, \\u201cIseeq: Information seeking question generation using dynamic meta-information retrieval and knowledge graphs,\\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 10, 2022, pp. 10 672\\u201310 680. [87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch\\u00a8arli, and D. Zhou, \\u201cLarge language models can be easily distracted by irrelevant context,\\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 31 210\\u201331 227. [88] R. Teja, \\u201cEvaluating the ideal chunk size for a rag system using llamaindex,\\u201d https://www.llamaindex.ai/blog/ evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5, 2023.\",\n          \"16 Fig. 6. Summary of RAG ecosystem initial learning curve. 3) Specialization - optimizing RAG to better serve production environments. The mutual growth of RAG models and their technology stacks is evident; technological advancements continuously establish new standards for existing infrastructure. In turn, enhancements to the technology stack drive the development of RAG capabilities. RAG toolkits are converging into a foundational technology stack, laying the groundwork for advanced enterprise applications. However, a fully integrated, comprehensive platform concept is still in the future, requiring further innovation and development. F. Multi-modal RAG RAG has transcended its initial text-based question- answering confines, embracing a diverse array of modal data. This expansion has spawned innovative multimodal models that integrate RAG concepts across various domains: Image. RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images. BLIP-2 [177] leverages frozen image encoders alongside LLMs for efficient visual language pre-training, enabling zero- shot image-to-text conversions. The \\u201cVisualize Before You Write\\u201d method [178] employs image generation to steer the LM\\u2019s text generation, showing promise in open-ended text generation tasks. Audio and Video. The GSS method retrieves and stitches together audio clips to convert machine-translated data into speech-translated data [179]. UEOP marks a significant ad- vancement in end-to-end automatic speech recognition by incorporating external, offline strategies for voice-to-text con- version [180]. Additionally, KNN-based attention fusion lever- ages audio embeddings and semantically related text embed- dings to refine ASR, thereby accelerating domain adaptation. Vid2Seq augments language models with specialized temporal markers, facilitating the prediction of event boundaries and textual descriptions within a unified output sequence [181]. Code. RBPS [182] excels in small-scale learning tasks by retrieving code examples that align with developers\\u2019 objectives through encoding and frequency analysis. This approach has demonstrated efficacy in tasks such as test assertion genera- tion and program repair. For structured knowledge, the CoK method [106] first extracts facts pertinent to the input query from a knowledge graph, then integrates these facts as hints within the input, enhancing performance in knowledge graph question-answering tasks. VIII. CONCLUSION The summary of this paper, as depicted in Figure 6, empha- sizes RAG\\u2019s significant advancement in enhancing the capa- bilities of LLMs by integrating parameterized knowledge from language models with extensive non-parameterized data from external knowledge bases. The survey showcases the evolution of RAG technologies and their application on many different tasks. The analysis outlines three developmental paradigms within the RAG framework: Naive, Advanced, and Modu- lar RAG, each representing a progressive enhancement over its predecessors. RAG\\u2019s technical integration with other AI methodologies, such as fine-tuning and reinforcement learning, has further expanded its capabilities. Despite the progress in RAG technology, there are research opportunities to improve its robustness and its ability to handle extended contexts. RAG\\u2019s application scope is expanding into multimodal do- mains, adapting its principles to interpret and process diverse data forms like images, videos, and code. This expansion high- lights RAG\\u2019s significant practical implications for AI deploy- ment, attracting interest from academic and industrial sectors.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-433b59aa-77c9-4128-9230-853ed95fd1cc\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5451</td>\n",
       "      <td>739</td>\n",
       "      <td>28</td>\n",
       "      <td>1362.75</td>\n",
       "      <td>1 Retrieval-Augmented Generation for Large Lan...</td>\n",
       "      <td>[(1, Retrieval, -, Augmented, Generation, for,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3082</td>\n",
       "      <td>450</td>\n",
       "      <td>32</td>\n",
       "      <td>770.50</td>\n",
       "      <td>2 Fig. 1. Technology tree of RAG research. The...</td>\n",
       "      <td>[(2, Fig, .), (1, .), (Technology, tree, of, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3950</td>\n",
       "      <td>575</td>\n",
       "      <td>41</td>\n",
       "      <td>987.50</td>\n",
       "      <td>3 Fig. 2. A representative instance of the RAG...</td>\n",
       "      <td>[(3, Fig, .), (2, .), (A, representative, inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3574</td>\n",
       "      <td>492</td>\n",
       "      <td>30</td>\n",
       "      <td>893.50</td>\n",
       "      <td>4 Fig. 3. Comparison between the three paradig...</td>\n",
       "      <td>[(4, Fig, .), (3, .), (Comparison, between, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6362</td>\n",
       "      <td>887</td>\n",
       "      <td>42</td>\n",
       "      <td>1590.50</td>\n",
       "      <td>5 aligns the text more closely with data distr...</td>\n",
       "      <td>[(5, aligns, the, text, more, closely, with, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-433b59aa-77c9-4128-9230-853ed95fd1cc')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-433b59aa-77c9-4128-9230-853ed95fd1cc button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-433b59aa-77c9-4128-9230-853ed95fd1cc');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-4d398fb4-c6ec-4ccb-a5ec-b506756586d8\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4d398fb4-c6ec-4ccb-a5ec-b506756586d8')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-4d398fb4-c6ec-4ccb-a5ec-b506756586d8 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0            0             5451              739                       28   \n",
       "1            1             3082              450                       32   \n",
       "2            2             3950              575                       41   \n",
       "3            3             3574              492                       30   \n",
       "4            4             6362              887                       42   \n",
       "\n",
       "   page_token_count                                               text  \\\n",
       "0           1362.75  1 Retrieval-Augmented Generation for Large Lan...   \n",
       "1            770.50  2 Fig. 1. Technology tree of RAG research. The...   \n",
       "2            987.50  3 Fig. 2. A representative instance of the RAG...   \n",
       "3            893.50  4 Fig. 3. Comparison between the three paradig...   \n",
       "4           1590.50  5 aligns the text more closely with data distr...   \n",
       "\n",
       "                                      sentences_list  \n",
       "0  [(1, Retrieval, -, Augmented, Generation, for,...  \n",
       "1  [(2, Fig, .), (1, .), (Technology, tree, of, R...  \n",
       "2  [(3, Fig, .), (2, .), (A, representative, inst...  \n",
       "3  [(4, Fig, .), (3, .), (Comparison, between, th...  \n",
       "4  [(5, aligns, the, text, more, closely, with, d...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentences_list'] = df['text'].apply(lambda text: list(nlp(text).sents))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN_vm16p2py5"
   },
   "source": [
    "From Page 16 onward are about the references which don't have much important information, so I decided to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "VSeUoY3m2py5"
   },
   "outputs": [],
   "source": [
    "df.drop([16, 17, 18, 19, 20], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLb-gt8C2py4"
   },
   "source": [
    "Since I plan to use `all-mpnet-base-v2` model which has a capacity of 384 tokens to embed the text.\n",
    "\n",
    "So, the text has to be splitted into chunks to make sure that they are not exceed the model's capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZTzYZDtA8R3"
   },
   "source": [
    "### Chunk sentences together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "y8VyNmdUjIeX"
   },
   "outputs": [],
   "source": [
    "# Create a split list function\n",
    "# e.g. [7] --> [5, 2]\n",
    "def split_list(input_list: list, slice_size: int = 5):\n",
    "    temp = []\n",
    "    # Convert dtype = spacy.tokens.span.Span to String\n",
    "    for sentence in input_list:\n",
    "        temp.append(sentence.text)\n",
    "    input_list = temp\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "ZEqJSvf7pFx0",
    "outputId": "6d594a56-100c-4d72-acbb-0542baf69278"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 15,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1292,\n        \"min\": 2260,\n        \"max\": 6362,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          5451,\n          3082,\n          3967\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 183,\n        \"min\": 340,\n        \"max\": 904,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          739,\n          450,\n          531\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 1,\n        \"max\": 54,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          45,\n          48,\n          28\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 323.1329572682376,\n        \"min\": 565.0,\n        \"max\": 1590.5,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          1362.75,\n          770.5,\n          991.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"1 Retrieval-Augmented Generation for Large Language Models: A Survey Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng Wangc, and Haofen Wang a,c aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University cCollege of Design and Innovation, Tongji University Abstract\\u2014Large Language Models (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain- specific information. RAG synergistically merges LLMs\\u2019 intrin- sic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evalua- tion framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development 1. Index Terms\\u2014Large language model, retrieval-augmented gen- eration, natural language processing, information retrieval I. INTRODUCTION L ARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks [1], notably producing \\u201challucinations\\u201d [2] when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applica- tions. RAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown Corresponding Author.Email:haofen.wang@tongji.edu.cn 1Resources are available at https://github.com/Tongji-KGLLM/ RAG-Survey in Figure 1. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG\\u2019s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating additional knowledge through Pre- Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques [3]\\u2013[5].The subsequent arrival of ChatGPT [6] marked a pivotal moment, with LLM demonstrating powerful in context learning (ICL) capabilities. RAG research shifted towards providing better information for LLMs to answer more com- plex and knowledge-intensive tasks during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques. The burgeoning field of RAG has experienced swift growth, yet it has not been accompanied by a systematic synthesis that could clarify its broader trajectory. This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of \\u201cRetrieval,\\u201d \\u201cGeneration,\\u201d and \\u201cAugmentation.\\u201d On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper compre- hensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: \\u2022 In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, arXiv:2312.10997v5  [cs.CL]  27 Mar 2024\",\n          \"2 Fig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs, research on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent research has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training stage through retrieval-augmented techniques. advanced RAG, and modular RAG. This review contex- tualizes the broader scope of RAG research within the landscape of LLMs. \\u2022 We identify and discuss the central technologies integral to the RAG process, specifically focusing on the aspects of \\u201cRetrieval\\u201d, \\u201cGeneration\\u201d and \\u201cAugmentation\\u201d, and delve into their synergies, elucidating how these com- ponents intricately collaborate to form a cohesive and effective RAG framework. \\u2022 We have summarized the current assessment methods of RAG, covering 26 tasks, nearly 50 datasets, outlining the evaluation objectives and metrics, as well as the current evaluation benchmarks and tools. Additionally, we anticipate future directions for RAG, emphasizing potential enhancements to tackle current challenges. The paper unfolds as follows: Section II introduces the main concept and current paradigms of RAG. The following three sections explore core components\\u2014\\u201cRetrieval\\u201d, \\u201cGen- eration\\u201d and \\u201cAugmentation\\u201d, respectively. Section III focuses on optimization methods in retrieval,including indexing, query and embedding optimization. Section IV concentrates on post- retrieval process and LLM fine-tuning in generation. Section V analyzes the three augmentation processes. Section VI focuses on RAG\\u2019s downstream tasks and evaluation system. Sec- tion VII mainly discusses the challenges that RAG currently faces and its future development directions. At last, the paper concludes in Section VIII. II. OVERVIEW OF RAG A typical application of RAG is illustrated in Figure 2. Here, a user poses a question to ChatGPT about a recent, widely discussed news. Given ChatGPT\\u2019s reliance on pre- training data, it initially lacks the capacity to provide up- dates on recent developments. RAG bridges this information gap by sourcing and incorporating knowledge from external databases. In this case, it gathers relevant news articles related to the user\\u2019s query. These articles, combined with the original question, form a comprehensive prompt that empowers LLMs to generate a well-informed answer. The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure 3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG. A. Naive RAG The Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the\",\n          \"6 TABLE I SUMMARY OF RAG METHODS Method Retrieval Source Retrieval Data Type Retrieval Granularity Augmentation Stage Retrieval process CoG [29] Wikipedia Text Phrase Pre-training Iterative DenseX [30] FactoidWiki Text Proposition Inference Once EAR [31] Dataset-base Text Sentence Tuning Once UPRISE [20] Dataset-base Text Sentence Tuning Once RAST [32] Dataset-base Text Sentence Tuning Once Self-Mem [17] Dataset-base Text Sentence Tuning Iterative FLARE [24] Search Engine,Wikipedia Text Sentence Tuning Adaptive PGRA [33] Wikipedia Text Sentence Inference Once FILCO [34] Wikipedia Text Sentence Inference Once RADA [35] Dataset-base Text Sentence Inference Once Filter-rerank [36] Synthesized dataset Text Sentence Inference Once R-GQA [37] Dataset-base Text Sentence Pair Tuning Once LLM-R [38] Dataset-base Text Sentence Pair Inference Iterative TIGER [39] Dataset-base Text Item-base Pre-training Once LM-Indexer [40] Dataset-base Text Item-base Tuning Once BEQUE [9] Dataset-base Text Item-base Tuning Once CT-RAG [41] Synthesized dataset Text Item-base Tuning Once Atlas [42] Wikipedia, Common Crawl Text Chunk Pre-training Iterative RAVEN [43] Wikipedia Text Chunk Pre-training Once RETRO++ [44] Pre-training Corpus Text Chunk Pre-training Iterative INSTRUCTRETRO [45] Pre-training corpus Text Chunk Pre-training Iterative RRR [7] Search Engine Text Chunk Tuning Once RA-e2e [46] Dataset-base Text Chunk Tuning Once PROMPTAGATOR [21] BEIR Text Chunk Tuning Once AAR [47] MSMARCO,Wikipedia Text Chunk Tuning Once RA-DIT [27] Common Crawl,Wikipedia Text Chunk Tuning Once RAG-Robust [48] Wikipedia Text Chunk Tuning Once RA-Long-Form [49] Dataset-base Text Chunk Tuning Once CoN [50] Wikipedia Text Chunk Tuning Once Self-RAG [25] Wikipedia Text Chunk Tuning Adaptive BGM [26] Wikipedia Text Chunk Inference Once CoQ [51] Wikipedia Text Chunk Inference Iterative Token-Elimination [52] Wikipedia Text Chunk Inference Once PaperQA [53] Arxiv,Online Database,PubMed Text Chunk Inference Iterative NoiseRAG [54] FactoidWiki Text Chunk Inference Once IAG [55] Search Engine,Wikipedia Text Chunk Inference Once NoMIRACL [56] Wikipedia Text Chunk Inference Once ToC [57] Search Engine,Wikipedia Text Chunk Inference Recursive SKR [58] Dataset-base,Wikipedia Text Chunk Inference Adaptive ITRG [59] Wikipedia Text Chunk Inference Iterative RAG-LongContext [60] Dataset-base Text Chunk Inference Once ITER-RETGEN [14] Wikipedia Text Chunk Inference Iterative IRCoT [61] Wikipedia Text Chunk Inference Recursive LLM-Knowledge-Boundary [62] Wikipedia Text Chunk Inference Once RAPTOR [63] Dataset-base Text Chunk Inference Recursive RECITE [22] LLMs Text Chunk Inference Once ICRALM [64] Pile,Wikipedia Text Chunk Inference Iterative Retrieve-and-Sample [65] Dataset-base Text Doc Tuning Once Zemi [66] C4 Text Doc Tuning Once CRAG [67] Arxiv Text Doc Inference Once 1-PAGER [68] Wikipedia Text Doc Inference Iterative PRCA [69] Dataset-base Text Doc Inference Once QLM-Doc-ranking [70] Dataset-base Text Doc Inference Once Recomp [71] Wikipedia Text Doc Inference Once DSP [23] Wikipedia Text Doc Inference Iterative RePLUG [72] Pile Text Doc Inference Once ARM-RAG [73] Dataset-base Text Doc Inference Iterative GenRead [13] LLMs Text Doc Inference Iterative UniMS-RAG [74] Dataset-base Text Multi Tuning Once CREA-ICL [19] Dataset-base Crosslingual,Text Sentence Inference Once PKG [75] LLM Tabular,Text Chunk Inference Once SANTA [76] Dataset-base Code,Text Item Pre-training Once SURGE [77] Freebase KG Sub-Graph Tuning Once MK-ToD [78] Dataset-base KG Entity Tuning Once Dual-Feedback-ToD [79] Dataset-base KG Entity Sequence Tuning Once KnowledGPT [15] Dataset-base KG Triplet Inference Muti-time FABULA [80] Dataset-base,Graph KG Entity Inference Once HyKGE [81] CMeKG KG Entity Inference Once KALMV [82] Wikipedia KG Triplet Inference Iterative RoG [83] Freebase KG Triplet Inference Iterative G-Retriever [84] Dataset-base TextGraph Sub-Graph Inference Once\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"splitted_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-b96ab996-205b-44fc-993b-6104f6b58912\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences_list</th>\n",
       "      <th>splitted_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5451</td>\n",
       "      <td>739</td>\n",
       "      <td>28</td>\n",
       "      <td>1362.75</td>\n",
       "      <td>1 Retrieval-Augmented Generation for Large Lan...</td>\n",
       "      <td>[(1, Retrieval, -, Augmented, Generation, for,...</td>\n",
       "      <td>[[1 Retrieval-Augmented Generation for Large L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3082</td>\n",
       "      <td>450</td>\n",
       "      <td>32</td>\n",
       "      <td>770.50</td>\n",
       "      <td>2 Fig. 1. Technology tree of RAG research. The...</td>\n",
       "      <td>[(2, Fig, .), (1, .), (Technology, tree, of, R...</td>\n",
       "      <td>[[2 Fig., 1., Technology tree of RAG research....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3950</td>\n",
       "      <td>575</td>\n",
       "      <td>41</td>\n",
       "      <td>987.50</td>\n",
       "      <td>3 Fig. 2. A representative instance of the RAG...</td>\n",
       "      <td>[(3, Fig, .), (2, .), (A, representative, inst...</td>\n",
       "      <td>[[3 Fig., 2., A representative instance of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3574</td>\n",
       "      <td>492</td>\n",
       "      <td>30</td>\n",
       "      <td>893.50</td>\n",
       "      <td>4 Fig. 3. Comparison between the three paradig...</td>\n",
       "      <td>[(4, Fig, .), (3, .), (Comparison, between, th...</td>\n",
       "      <td>[[4 Fig., 3., Comparison between the three par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6362</td>\n",
       "      <td>887</td>\n",
       "      <td>42</td>\n",
       "      <td>1590.50</td>\n",
       "      <td>5 aligns the text more closely with data distr...</td>\n",
       "      <td>[(5, aligns, the, text, more, closely, with, d...</td>\n",
       "      <td>[[5 aligns the text more closely with data dis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b96ab996-205b-44fc-993b-6104f6b58912')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-b96ab996-205b-44fc-993b-6104f6b58912 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-b96ab996-205b-44fc-993b-6104f6b58912');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-45be7019-e4a0-418f-b8be-224976f4b333\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-45be7019-e4a0-418f-b8be-224976f4b333')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-45be7019-e4a0-418f-b8be-224976f4b333 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0            0             5451              739                       28   \n",
       "1            1             3082              450                       32   \n",
       "2            2             3950              575                       41   \n",
       "3            3             3574              492                       30   \n",
       "4            4             6362              887                       42   \n",
       "\n",
       "   page_token_count                                               text  \\\n",
       "0           1362.75  1 Retrieval-Augmented Generation for Large Lan...   \n",
       "1            770.50  2 Fig. 1. Technology tree of RAG research. The...   \n",
       "2            987.50  3 Fig. 2. A representative instance of the RAG...   \n",
       "3            893.50  4 Fig. 3. Comparison between the three paradig...   \n",
       "4           1590.50  5 aligns the text more closely with data distr...   \n",
       "\n",
       "                                      sentences_list  \\\n",
       "0  [(1, Retrieval, -, Augmented, Generation, for,...   \n",
       "1  [(2, Fig, .), (1, .), (Technology, tree, of, R...   \n",
       "2  [(3, Fig, .), (2, .), (A, representative, inst...   \n",
       "3  [(4, Fig, .), (3, .), (Comparison, between, th...   \n",
       "4  [(5, aligns, the, text, more, closely, with, d...   \n",
       "\n",
       "                                       splitted_list  \n",
       "0  [[1 Retrieval-Augmented Generation for Large L...  \n",
       "1  [[2 Fig., 1., Technology tree of RAG research....  \n",
       "2  [[3 Fig., 2., A representative instance of the...  \n",
       "3  [[4 Fig., 3., Comparison between the three par...  \n",
       "4  [[5 aligns the text more closely with data dis...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['splitted_list'] = df['sentences_list'].apply(split_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMz-SUj7sA8G"
   },
   "source": [
    "### Create new DataFrame that contains only chunks of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "VvN5JL_nrOXu",
    "outputId": "cfb4a773-13e8-4944-b648-064433407dd7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"sentences_df\",\n  \"rows\": 106,\n  \"fields\": [\n    {\n      \"column\": \"page\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 15,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 106.46316893577453,\n        \"min\": 17.25,\n        \"max\": 991.75,\n        \"num_unique_values\": 98,\n        \"samples\": [\n          275.0,\n          102.75,\n          138.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 106,\n        \"samples\": [\n          \"16 Fig.6.Summary of RAG ecosystem initial learning curve.3) Specialization - optimizing RAG to better serve production environments.The mutual growth of RAG models and their technology stacks is evident; technological advancements continuously establish new standards for existing infrastructure.\",\n          \"II.OVERVIEW OF RAG A typical application of RAG is illustrated in Figure 2.Here, a user poses a question to ChatGPT about a recent, widely discussed news.Given ChatGPT\\u2019s reliance on pre- training data, it initially lacks the capacity to provide up- dates on recent developments.RAG bridges this information gap by sourcing and incorporating knowledge from external databases.\",\n          \"This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs.This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of \\u201cRetrieval,\\u201d \\u201cGeneration,\\u201d and \\u201cAugmentation.\\u201dOn the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG.This paper compre- hensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG.Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "sentences_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-48265c5c-650b-4551-82e3-2c69f1283028\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>token_count</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>303.25</td>\n",
       "      <td>1 Retrieval-Augmented Generation for Large Lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>243.25</td>\n",
       "      <td>It meticulously scrutinizes the tripartite fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>212.50</td>\n",
       "      <td>To overcome challenges, Retrieval-Augmented Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>235.25</td>\n",
       "      <td>Initially, RAGs inception coincided with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>218.50</td>\n",
       "      <td>This survey endeavors to fill this gap by mapp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48265c5c-650b-4551-82e3-2c69f1283028')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-48265c5c-650b-4551-82e3-2c69f1283028 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-48265c5c-650b-4551-82e3-2c69f1283028');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-642522f2-5aff-48a6-b8f9-3952b90ef654\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-642522f2-5aff-48a6-b8f9-3952b90ef654')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-642522f2-5aff-48a6-b8f9-3952b90ef654 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   page  token_count                                          sentences\n",
       "0     0       303.25  1 Retrieval-Augmented Generation for Large Lan...\n",
       "0     0       243.25  It meticulously scrutinizes the tripartite fou...\n",
       "0     0       212.50  To overcome challenges, Retrieval-Augmented Ge...\n",
       "0     0       235.25  Initially, RAGs inception coincided with the ...\n",
       "0     0       218.50  This survey endeavors to fill this gap by mapp..."
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df = df.explode('splitted_list')\n",
    "sentences_df['splitted_list'] = sentences_df['splitted_list'].apply(lambda x: \"\".join(x))\n",
    "sentences_df['token_count'] = sentences_df['splitted_list'].apply(lambda x: len(x) / 4)\n",
    "sentences_df.rename(columns={'page_number': 'page', 'splitted_list': 'sentences'}, inplace=True)\n",
    "sentences_df = sentences_df[['page', 'token_count', 'sentences']]\n",
    "sentences_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czsnrp687675"
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ylBkaSMc2py5",
    "outputId": "d00211e2-9cc8-4a09-fa7d-50ca96a7c1f8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"sentences_df\",\n  \"rows\": 106,\n  \"fields\": [\n    {\n      \"column\": \"page\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 15,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 106.46316893577453,\n        \"min\": 17.25,\n        \"max\": 991.75,\n        \"num_unique_values\": 98,\n        \"samples\": [\n          275.0,\n          102.75,\n          138.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 106,\n        \"samples\": [\n          \"16 Fig.6.Summary of RAG ecosystem initial learning curve.3) Specialization - optimizing RAG to better serve production environments.The mutual growth of RAG models and their technology stacks is evident; technological advancements continuously establish new standards for existing infrastructure.\",\n          \"II.OVERVIEW OF RAG A typical application of RAG is illustrated in Figure 2.Here, a user poses a question to ChatGPT about a recent, widely discussed news.Given ChatGPT\\u2019s reliance on pre- training data, it initially lacks the capacity to provide up- dates on recent developments.RAG bridges this information gap by sourcing and incorporating knowledge from external databases.\",\n          \"This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs.This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of \\u201cRetrieval,\\u201d \\u201cGeneration,\\u201d and \\u201cAugmentation.\\u201dOn the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG.This paper compre- hensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG.Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "sentences_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-04d1264a-f339-42a8-97c7-d064c9fbdf34\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>token_count</th>\n",
       "      <th>sentences</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>303.25</td>\n",
       "      <td>1 Retrieval-Augmented Generation for Large Lan...</td>\n",
       "      <td>[0.051494826, 0.048605476, -0.040616732, 0.040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>243.25</td>\n",
       "      <td>It meticulously scrutinizes the tripartite fou...</td>\n",
       "      <td>[0.078784674, 0.02718093, -0.0130607085, 0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>212.50</td>\n",
       "      <td>To overcome challenges, Retrieval-Augmented Ge...</td>\n",
       "      <td>[0.061909616, -0.03423412, -0.010302416, 0.023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>235.25</td>\n",
       "      <td>Initially, RAGs inception coincided with the ...</td>\n",
       "      <td>[0.03596034, 0.011139151, -0.017413778, 0.0121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>218.50</td>\n",
       "      <td>This survey endeavors to fill this gap by mapp...</td>\n",
       "      <td>[0.031225106, 0.018977925, -0.0044133575, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04d1264a-f339-42a8-97c7-d064c9fbdf34')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-04d1264a-f339-42a8-97c7-d064c9fbdf34 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-04d1264a-f339-42a8-97c7-d064c9fbdf34');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-22e33ba6-134c-4089-a73d-e00a055d2d16\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-22e33ba6-134c-4089-a73d-e00a055d2d16')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-22e33ba6-134c-4089-a73d-e00a055d2d16 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   page  token_count                                          sentences  \\\n",
       "0     0       303.25  1 Retrieval-Augmented Generation for Large Lan...   \n",
       "0     0       243.25  It meticulously scrutinizes the tripartite fou...   \n",
       "0     0       212.50  To overcome challenges, Retrieval-Augmented Ge...   \n",
       "0     0       235.25  Initially, RAGs inception coincided with the ...   \n",
       "0     0       218.50  This survey endeavors to fill this gap by mapp...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.051494826, 0.048605476, -0.040616732, 0.040...  \n",
       "0  [0.078784674, 0.02718093, -0.0130607085, 0.013...  \n",
       "0  [0.061909616, -0.03423412, -0.010302416, 0.023...  \n",
       "0  [0.03596034, 0.011139151, -0.017413778, 0.0121...  \n",
       "0  [0.031225106, 0.018977925, -0.0044133575, -0.0...  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                      device=device)\n",
    "\n",
    "\n",
    "sentences_df['embedding'] = sentences_df['sentences'].apply(lambda sentences: embedding_model.encode(sentences))\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mn-HrqmmCxfc"
   },
   "source": [
    "# 2. RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwMgoktU7zZb",
    "outputId": "dcf5772d-1d0d-48c0-fb50-54a9e9932e89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([106, 768])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "# Convert embeddings to torch tensor\n",
    "embeddings = torch.tensor(np.array(sentences_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JcG00PzJ-7I"
   },
   "source": [
    "### Sematic Search\n",
    "\n",
    "Steps:\n",
    "1. Define a query string\n",
    "2. Turn the query string into an embedding with same model.\n",
    "3. Perform a `dot product` or `cosine similarity` to get similarity scores.\n",
    "4. Retrieve the highest score from Step 3\n",
    "\n",
    "This is the part where we try to search for:\n",
    "\n",
    "![query1.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJPxSM4oEHJT",
    "outputId": "8042d736-f313-4b51-c96d-3631105b59c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.6247, 0.5097, 0.4903, 0.4871, 0.4640], device='cuda:0'),\n",
       "indices=tensor([12, 21, 18, 22, 11], device='cuda:0'))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Define a query string\n",
    "query = \"RAG paradigms\"\n",
    "\n",
    "# 2. Turn the query string into an embedding with same model.\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# 3. Perform a `dot product` or `cosine similarity` to get similarity scores.\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "\n",
    "# 4. Retrieve the highest score from Step 3\n",
    "dot_product_results = torch.topk(dot_scores, k=5) # k=5 means top 5\n",
    "dot_product_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AeTgBtApLQOR",
    "outputId": "d3d05e76-e6ab-49f4-aa88-1bc8b70af6b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score = 0.625\n",
      " Sentence = A. Naive RAG The Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the\n",
      "\n",
      "Score = 0.510\n",
      " Sentence = 4 Fig.3.Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle) Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a chain-like structure. (\n",
      "\n",
      "Score = 0.490\n",
      " Sentence = However, Naive RAG encounters notable drawbacks: Retrieval Challenges.The retrieval phase often struggles with precision and recall, leading to the selection of misaligned or irrelevant chunks, and the missing of crucial information.Generation Difficulties.In generating responses, the model may face the issue of hallucination, where it produces con- tent not supported by the retrieved context.This phase can also suffer from irrelevance, toxicity, or bias in the outputs, detracting from the quality and reliability of the responses.\n",
      "\n",
      "Score = 0.487\n",
      " Sentence = Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall.This is evident in the introduction of multiple specific functional modules and the replacement of existing modules.The overall process is not limited to sequential retrieval and generation; it includes methods such as iterative and adaptive retrieval.Pre-retrieval process.In this stage, the primary focus is on optimizing the indexing structure and the original query.\n",
      "\n",
      "Score = 0.464\n",
      " Sentence = In this case, it gathers relevant news articles related to the users query.These articles, combined with the original question, form a comprehensive prompt that empowers LLMs to generate a well-informed answer.The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure 3.Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations.The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for score, idx in zip(dot_product_results[0], dot_product_results[1]):\n",
    "    print(f\"Score = {score:.3f}\\n Sentence = {sentences_df.iloc[idx.item()]['sentences']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Vo-XRVQSR7H"
   },
   "source": [
    "## Functionizing the semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "twt4koABW_To"
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                top: int=5):\n",
    "    # Turn the query string into an embedding\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    # Perform a `dot product` to get similarity scores.\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    # Retrieve the highest score\n",
    "    scores, indices = torch.topk(input=dot_scores, k=top)\n",
    "\n",
    "    return scores, indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8Psj0DOSdMo"
   },
   "source": [
    "# 3. Load a Local LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPO7iJpfSoDX"
   },
   "source": [
    "Based on the website below, it requires 8 GB minimum VRAM to use Gemma 2B and 24 GB for Gemma 7B model.\n",
    "\n",
    "**Ref:** https://github.com/google-deepmind/gemma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyvSA6OsRm91",
    "outputId": "b2b4fe80-29da-4794-9912-ee152c38ca94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My GPU memory: 40 GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "    gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "    print(f\"My GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMwV_Q7Tf8_G"
   },
   "source": [
    "### Get Hugging Face's token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "5cJQSaRAb_Wa"
   },
   "outputs": [],
   "source": [
    "# Run on Google Colab\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HUGGINGFACE_TOKEN')\n",
    "# Run on local\n",
    "else:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(override=True)\n",
    "    hf_token = os.getenv('HUGGINGFACE_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500,
     "referenced_widgets": [
      "cbba871822d54c3ea8edf6d6c43f3431",
      "47707bd1887c40cda29cccdbb82b2b15",
      "08e18c9c623b4813a107e8487187494d",
      "e1f9a35877c94be6a1aec249535839ae",
      "1976beec918140db8c6de3ce38346b02",
      "70f87a090ebf475cab987200c6dd4c3e",
      "c85ff313afa9451da11bfb2e7a84cc48",
      "ffa100d951cf49589b2b71236b0d66fe",
      "812c29ac25ff48bbb852cd1161ec4bbf",
      "fb3742fef16c4587bd1161aa432d98c3",
      "49dcca7cc07c49eba3bdb46994edae44"
     ]
    },
    "id": "FCxl1W7mUOMr",
    "outputId": "fbc7cad4-3a54-4eb0-90f1-d73bcf736a09"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbba871822d54c3ea8edf6d6c43f3431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# 1. Create quantization config for smaller model loading\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# 2. Define Model ID\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
    "                                          token=hf_token)\n",
    "\n",
    "# 4. Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 quantization_config=quantization_config,\n",
    "                                                 device_map='auto',\n",
    "                                                 token=hf_token,\n",
    "                                                 attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUaRlE9DSw7E"
   },
   "source": [
    "### Text Generation using local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSwL_MPaj7xc",
    "outputId": "ddb2826c-e4f7-4ba1-911f-9b374af2f5f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>What is Retrieval Augmented Generation (RAG) ?\n",
      "\n",
      "Retrieval Augmented Generation (RAG) is a machine learning technique that combines the strengths of retrieval and generation models to achieve state-of-the-art performance on various tasks.\n",
      "\n",
      "**Here's how it works:**\n",
      "\n",
      "1. **Retrieval:** A pre-trained model is used to retrieve relevant information from a large dataset. This information can be text, code, images, or other types of data.\n",
      "2. **Generation:** The retrieved information is then used as a prompt to generate new content, such as text, code, images, or other types of data.\n",
      "\n",
      "**Benefits of RAG:**\n",
      "\n",
      "* **Improved performance:** RAG can achieve state-of-the-art performance on various tasks, including text generation, code generation, and image generation.\n",
      "* **Robustness:** RAG is robust to noise and variations in the input data.\n",
      "* **Interpretability:** The generated content can be explained by the pre-trained retrieval model.\n",
      "\n",
      "**Applications of RAG:**\n",
      "\n",
      "* **Natural language processing (NLP):** Generating text, translating languages, and summarizing text.\n",
      "* **Code generation:** Generating code in various programming languages.\n",
      "* **Image generation:** Creating new images based on existing data.\n",
      "* **Multimodal generation:** Generating content in multiple\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What is Retrieval Augmented Generation (RAG) ?\"\n",
    "\n",
    "# Create chat template\n",
    "template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Create Prompt\n",
    "prompt = tokenizer.apply_chat_template(conversation=template,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True,\n",
    "                                       chat_template=input_text\n",
    "                                       )\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate outputs passed on the tokenized input\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             max_new_tokens=256)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLFYllWGfN7n"
   },
   "source": [
    "### Format the Prompt to ask LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "DketiomFVS7n"
   },
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, context_items: list) -> str:\n",
    "\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item for item in context_items])\n",
    "\n",
    "    # Create a base prompt with contexts from RAG\n",
    "    base_prompt = \"\"\"Your task is to act as a knowledgeable assistant that answers queries based on the provided context.\n",
    "    Below, you will find a query followed by relevant excerpts retrieved from a knowledge base.\n",
    "    Use the information from these excerpts to construct a clear, concise, and accurate response. Do not fabricate information;\n",
    "    if the context lacks the required details to fully answer the query, indicate that the answer is incomplete and explain the limitations based on the available context.\n",
    "    The response should be professional and adhere to the source's details.\\n\n",
    "\n",
    "    Example Structure\n",
    "    Query: {query}\n",
    "\n",
    "    Context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Your Response:\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query\n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=template,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True,\n",
    "                                       chat_template=base_prompt\n",
    "                                       )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m3kz2gWvYyZZ",
    "outputId": "ffb2493e-1fdf-427c-ee37-57a1689f6d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: RAG paradigm\n",
      "Your task is to act as a knowledgeable assistant that answers queries based on the provided context. \n",
      "    Below, you will find a query followed by relevant excerpts retrieved from a knowledge base. \n",
      "    Use the information from these excerpts to construct a clear, concise, and accurate response. Do not fabricate information; \n",
      "    if the context lacks the required details to fully answer the query, indicate that the answer is incomplete and explain the limitations based on the available context. \n",
      "    The response should be professional and adhere to the source's details.\n",
      "\n",
      "    \n",
      "    Example Structure\n",
      "    Query: RAG paradigm\n",
      "    \n",
      "    Context:\n",
      "    \n",
      "    - A. Naive RAG The Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the\n",
      "- However, Naive RAG encounters notable drawbacks: Retrieval Challenges.The retrieval phase often struggles with precision and recall, leading to the selection of misaligned or irrelevant chunks, and the missing of crucial information.Generation Difficulties.In generating responses, the model may face the issue of hallucination, where it produces con- tent not supported by the retrieved context.This phase can also suffer from irrelevance, toxicity, or bias in the outputs, detracting from the quality and reliability of the responses.\n",
      "- 4 Fig.3.Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle) Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a chain-like structure. (\n",
      "- Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall.This is evident in the introduction of multiple specific functional modules and the replacement of existing modules.The overall process is not limited to sequential retrieval and generation; it includes methods such as iterative and adaptive retrieval.Pre-retrieval process.In this stage, the primary focus is on optimizing the indexing structure and the original query.\n",
      "- 3 Fig.2.A representative instance of the RAG process applied to question answering.It mainly consists of 3 steps.1) Indexing.\n",
      "\n",
      "    Your Response:\n"
     ]
    }
   ],
   "source": [
    "query = 'RAG paradigm'\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get relevant resources\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "\n",
    "# Create a list of context items\n",
    "context_items = [sentences_df.iloc[i.item()]['sentences'] for i in indices]\n",
    "\n",
    "# Get the Prompt\n",
    "print(prompt_formatter(query, context_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_dR2SuAfd3U"
   },
   "source": [
    "### Create Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "lsTrTZtPfhdY"
   },
   "outputs": [],
   "source": [
    "def chat(query, temperature=0.7, max_new_tokens=256):\n",
    "\n",
    "    # Get just the scores and indices of top related results\n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings)\n",
    "\n",
    "    # Create a list of context items\n",
    "    context_items = [sentences_df.iloc[i.item()]['sentences'] for i in indices]\n",
    "\n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = llm_model.generate(**input,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    # Replace unnecessary help messages\n",
    "    output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7bLj3TEuUtr"
   },
   "source": [
    "# Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ACzB5EWgZ_8",
    "outputId": "89aa738c-f11b-4b15-eb3f-fdc72772a82e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\n",
      "\n",
      "The retrieval process in Naive RAG consists of three main stages: indexing, retrieval, and generation. The details of each stage are as follows:\n",
      "\n",
      "- Indexing involves optimizing the indexing structure and the original query to enhance the retrieval process.\n",
      "- Retrieval involves searching for relevant chunks within the indexed corpus based on the query using a chain-like structure.\n",
      "- Generation involves generating new content based on the retrieved chunks.\n",
      "\n",
      " Contexts:\n",
      "['11 Fig.5.In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves alternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval involves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval and generation. (', 'Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall.This is evident in the introduction of multiple specific functional modules and the replacement of existing modules.The overall process is not limited to sequential retrieval and generation; it includes methods such as iterative and adaptive retrieval.Pre-retrieval process.In this stage, the primary focus is on optimizing the indexing structure and the original query.', '4 Fig.3.Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle) Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a chain-like structure. (', 'In the RAG system, both the step-back question and the original query are used for retrieval, and both the results are utilized as the basis for language model answer generation.3) Query Routing: Based on varying queries, routing to distinct RAG pipeline,which is suitable for a versatile RAG system designed to accommodate diverse scenarios.Metadata Router/ Filter.The first step involves extracting keywords (entity) from the query, followed by filtering based on the keywords and metadata within the chunks to narrow down the search scope.Semantic Router is another method of routing involves leveraging the semantic information of the query.', 'This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase.Retrieval.Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation.It then computes the similarity scores between the query vector and the vector of chunks within the indexed corpus.The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query.']\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the steps involved in the retrieval process as described for Naive RAG?\"\n",
    "output_text, context_items = chat(query)\n",
    "print(f\"Response:\\n{output_text}\\n\\n Contexts:\\n{context_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save LLM's responses to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "LoeHIKgfwtNr"
   },
   "outputs": [],
   "source": [
    "query_list = ['What are the steps involved in the retrieval process as described for Naive RAG?',\n",
    "              'What are the three main paradigms of RAG mentioned in the document, and how do they differ?',\n",
    "              'What are the modular components introduced in Modular RAG, and how do they enhance system flexibility?',\n",
    "              'What are the anticipated research trends for handling super-long contexts in RAG systems?',\n",
    "              'How can RAG systems be tailored for domain-specific tasks, such as medical or legal question answering?']\n",
    "\n",
    "final_result = []\n",
    "\n",
    "for question in query_list:\n",
    "    chat_log = {}\n",
    "    output_text, context_items = chat(question)\n",
    "    chat_log['query'] = question\n",
    "    chat_log['relevant_contexts'] = context_items\n",
    "    chat_log['llm_response'] = output_text\n",
    "    final_result.append(chat_log)\n",
    "\n",
    "final_result_df = pd.DataFrame(final_result)\n",
    "final_result_df.to_csv('final_result.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "new_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08e18c9c623b4813a107e8487187494d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffa100d951cf49589b2b71236b0d66fe",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_812c29ac25ff48bbb852cd1161ec4bbf",
      "value": 2
     }
    },
    "1976beec918140db8c6de3ce38346b02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47707bd1887c40cda29cccdbb82b2b15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70f87a090ebf475cab987200c6dd4c3e",
      "placeholder": "",
      "style": "IPY_MODEL_c85ff313afa9451da11bfb2e7a84cc48",
      "value": "Loadingcheckpointshards:100%"
     }
    },
    "49dcca7cc07c49eba3bdb46994edae44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70f87a090ebf475cab987200c6dd4c3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "812c29ac25ff48bbb852cd1161ec4bbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c85ff313afa9451da11bfb2e7a84cc48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cbba871822d54c3ea8edf6d6c43f3431": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_47707bd1887c40cda29cccdbb82b2b15",
       "IPY_MODEL_08e18c9c623b4813a107e8487187494d",
       "IPY_MODEL_e1f9a35877c94be6a1aec249535839ae"
      ],
      "layout": "IPY_MODEL_1976beec918140db8c6de3ce38346b02"
     }
    },
    "e1f9a35877c94be6a1aec249535839ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb3742fef16c4587bd1161aa432d98c3",
      "placeholder": "",
      "style": "IPY_MODEL_49dcca7cc07c49eba3bdb46994edae44",
      "value": "2/2[00:03&lt;00:00,1.59s/it]"
     }
    },
    "fb3742fef16c4587bd1161aa432d98c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffa100d951cf49589b2b71236b0d66fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
